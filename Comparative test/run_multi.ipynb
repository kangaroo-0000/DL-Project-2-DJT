{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0187ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AGNEWS dataset...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Splitting dataset into training and evaluation sets...\n",
      "============================================================\n",
      "Running training with LoRA r=13, alpha=13, dropout=0.1\n",
      "============================================================\n",
      "trainable params: 993,028 || all params: 125,641,736 || trainable%: 0.7904\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Current eval_accuracy: 0.2437\n",
      "** Best so far! Saving current best model (accuracy=0.2437)\n",
      "\n",
      "Saved all runs' results to all_lora_results.csv\n",
      "    r  lora_alpha  lora_dropout  eval_accuracy\n",
      "0  13          13           0.1        0.24375\n",
      "\n",
      "============================================================\n",
      "All runs finished!\n",
      "Best accuracy=0.2437 with LoRA config: r=13, alpha=13, dropout=0.1\n",
      "============================================================\n",
      "Reloading best model from base + LoRA config, and verifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-check final metrics: {'accuracy': 0.24375}\n",
      "Final best model saved to best_lora_model\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Fine-Tuning RoBERTa with Multiple LoRA Hyperparameter Combinations\n",
    "for AG News text classification, then saving ALL run results and the best model.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Today's Date]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  1) 加载数据集 & 分词器\n",
    "# ==============================\n",
    "base_model = 'roberta-base'\n",
    "print(\"Loading AGNEWS dataset...\")\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 提取标签信息\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Label names: {class_names}\")\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# 定义 DataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  2) 切分训练集 / 验证集\n",
    "# ==============================\n",
    "print(\"Splitting dataset into training and evaluation sets...\")\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  3) 定义 LoRA 注入的模块\n",
    "# ==============================\n",
    "# 下面示例仅针对顶层若干模块，可根据实际需要进行调整\n",
    "target_modules = [\n",
    "    \"roberta.encoder.layer.7.attention.self.query\",\n",
    "    \"roberta.encoder.layer.7.attention.self.value\",\n",
    "    \"roberta.encoder.layer.8.attention.self.query\",\n",
    "    \"roberta.encoder.layer.8.attention.self.value\",\n",
    "    \"roberta.encoder.layer.9.attention.self.query\",\n",
    "    \"roberta.encoder.layer.9.attention.self.value\",\n",
    "    \"roberta.encoder.layer.10.intermediate.dense\",\n",
    "    \"roberta.encoder.layer.10.output.dense\",\n",
    "    \"roberta.encoder.layer.10.attention.self.query\",\n",
    "    \"roberta.encoder.layer.10.attention.self.value\",\n",
    "    \"roberta.encoder.layer.11.intermediate.dense\",\n",
    "    \"roberta.encoder.layer.11.output.dense\",\n",
    "    \"roberta.encoder.layer.11.attention.self.query\",\n",
    "    \"roberta.encoder.layer.11.attention.self.value\"\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  4) 训练与评估的通用设置\n",
    "# ==============================\n",
    "output_dir = \"results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-6,\n",
    "    num_train_epochs=0.1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.05,\n",
    "    warmup_steps=300,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset. \n",
    "    Returns (metrics, all_predictions) if labelled=True, else only predictions.\n",
    "    \"\"\"\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        metric = evaluate.load('accuracy')\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        if labelled:\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  5) 多组合 LoRA 超参搜索\n",
    "# ==============================\n",
    "# 这是你给出的组合\n",
    "lora_combinations = [\n",
    "    (13, 13, 0.1),\n",
    "    # (13, 13, 0.2),\n",
    "    # (13, 13, 0.3),\n",
    "    # (13, 26, 0.2),\n",
    "    # (13, 26, 0.3),\n",
    "    # (13, 39, 0.1),\n",
    "    # (13, 39, 0.2),\n",
    "    # (13, 39, 0.3),\n",
    "]\n",
    "\n",
    "# 用于记录每次 (r, alpha, dropout) 对应的评估结果\n",
    "all_results = []\n",
    "\n",
    "best_accuracy = -1\n",
    "best_config = None\n",
    "best_model_state = None  # 用于存储最佳模型权重\n",
    "\n",
    "for (r, alpha, dropout) in lora_combinations:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Running training with LoRA r={r}, alpha={alpha}, dropout={dropout}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1) 重新加载 base model\n",
    "    base_roberta = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        id2label=id2label,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    # 2) 构造对应的 LoRA 配置\n",
    "    peft_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "        bias='none',\n",
    "        target_modules=target_modules,\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "\n",
    "    # 3) 将基础模型包装为 LoRA 模型\n",
    "    peft_model = get_peft_model(base_roberta, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # 4) 实例化 Trainer\n",
    "    trainer = get_trainer(peft_model)\n",
    "\n",
    "    # 5) 开始训练\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(\"Finished training!\")\n",
    "\n",
    "    # 6) 评估\n",
    "    eval_result = trainer.evaluate()\n",
    "    current_accuracy = eval_result[\"eval_accuracy\"]\n",
    "    print(f\"--> Current eval_accuracy: {current_accuracy:.4f}\")\n",
    "\n",
    "    # 将该组合的结果记录下来\n",
    "    all_results.append({\n",
    "        \"r\": r,\n",
    "        \"lora_alpha\": alpha,\n",
    "        \"lora_dropout\": dropout,\n",
    "        \"eval_accuracy\": current_accuracy\n",
    "    })\n",
    "\n",
    "    # 若本组合优于先前最佳，则更新最优模型\n",
    "    if current_accuracy > best_accuracy:\n",
    "        best_accuracy = current_accuracy\n",
    "        best_config = (r, alpha, dropout)\n",
    "        print(f\"** Best so far! Saving current best model (accuracy={best_accuracy:.4f})\")\n",
    "        trainer.save_model(\"temp_best_model\")  \n",
    "        best_model_state = peft_model.state_dict()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  6) 保存所有组合的结果 (CSV)\n",
    "# ==============================\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.sort_values(by=\"eval_accuracy\", ascending=False, inplace=True)\n",
    "csv_path = \"all_lora_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved all runs' results to {csv_path}\")\n",
    "print(df_results)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "#  7) 加载最佳模型 & 最终验证\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All runs finished!\")\n",
    "print(f\"Best accuracy={best_accuracy:.4f} with LoRA config: r={best_config[0]}, alpha={best_config[1]}, dropout={best_config[2]}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Reloading best model from base + LoRA config, and verifying...\")\n",
    "\n",
    "# 1) 重新加载 base\n",
    "final_base = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# 2) 重新构造最佳 LoRA 配置\n",
    "final_peft_config = LoraConfig(\n",
    "    r=best_config[0],\n",
    "    lora_alpha=best_config[1],\n",
    "    lora_dropout=best_config[2],\n",
    "    bias='none',\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "final_peft_model = get_peft_model(final_base, final_peft_config)\n",
    "\n",
    "# 3) 加载最佳权重\n",
    "final_peft_model.load_state_dict(best_model_state)\n",
    "\n",
    "# 4) 最终验证\n",
    "metrics, _ = evaluate_model(\n",
    "    final_peft_model,\n",
    "    eval_dataset,\n",
    "    labelled=True,\n",
    "    batch_size=64,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "print(\"Double-check final metrics:\", metrics)\n",
    "\n",
    "# 5) 保存最终最佳模型\n",
    "final_save_path = \"best_lora_model\"\n",
    "final_peft_model.save_pretrained(final_save_path)\n",
    "tokenizer.save_pretrained(final_save_path)\n",
    "print(f\"Final best model saved to {final_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7992f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61408284dcbd465c94c03db1b8231028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 250/250 [00:15<00:00, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to inference_output.csv\n",
      "Finished all steps. You can now run inference or further analysis as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Optional: Inference on an Unlabelled Dataset\n",
    "# -------------------------------\n",
    "# Load an unlabelled dataset (example using pickle), tokenize, and run inference.\n",
    "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 加载 adapter 的 config\n",
    "config = PeftConfig.from_pretrained(\"best_adapter\")\n",
    "\n",
    "# 加载 base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    num_labels=4  # ✅ 必须与训练时一致\n",
    ")\n",
    "# 加载最佳 adapter 权重\n",
    "best_model = PeftModel.from_pretrained(base_model, \"best_adapter\")\n",
    "\n",
    "# 推理时使用这个模型\n",
    "preds = evaluate_model(best_model, test_dataset, labelled=False, batch_size=32, data_collator=data_collator)\n",
    "\n",
    "# preds = evaluate_model(peft_model, test_dataset, labelled=False, batch_size=32, data_collator=data_collator)\n",
    "# Save predictions to CSV.\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir, \"inference_output_B.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Finished all steps. You can now run inference or further analysis as needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
